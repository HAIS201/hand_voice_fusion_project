손 제스처와 음성 인식을 활용한 다중 모달 융합 연구

📌 프로젝트 소개

게임, VR, 교육 등의 분야에서 인간–컴퓨터 상호작용(HCI)은 점점 더 복잡해지고 있으며, 사용자는 단순한 키보드·마우스 입력이 아니라 신체 동작과 음성을 동시에 활용한 자연스러운 표현 방식을 요구하고 있다. 손 제스처는 사용자의 공간적 동작과 방향을 직관적으로 표현하는 데 강점을 가지고 있고, 음성은 “공격해”, “방어해”와 같은 고수준 추상 명령을 빠르게 전달하는 데 적합하다.

따라서 두 가지 모달리티(손 + 음성)를 함께 사용하는 다중 모달 융합 방식은 기존의 단일 모달 인터페이스보다 더 직관적이고 몰입감 있는 사용자 경험을 제공할 수 있다.

본 프로젝트에서는 웹캠과 마이크를 이용해
오른손 제스처로 이동(앞/뒤/좌/우/정지)을, 왼손 제스처와 음성으로 행동(공격/방어)을 제어하는 Unity 데모를 구현하고, 동일한 데이터셋에서 조기 융합(Early Fusion)과 후기 융합(Late Fusion) GRU 모델을 학습하여 두 융합 방식의 차이와 특성을 비교·분석하는 것을 목표로 한다.

🔎 동작 과정

1️⃣ 데이터 수집 & 라벨링

직접 촬영한 영상을 세 가지 subset으로 구성

A_hand : 손 제스처만 존재 (이동 중심)

B_voice: 음성만 존재 (ATTACK/DEFEND)

C_fusion: 손 + 음성이 동시에 존재 (이동 + 행동 페어)

밝기/소음 조합에 따라 E1~E4 환경 라벨을 부여

labels.csv 및 splits/train|val|test.csv로 메타데이터 관리

2️⃣ 특징 추출 (전처리)

손 제스처 (gesture)

MediaPipe Hands → 21개 랜드마크 추출

손목 기준 평행이동 + 손바닥 너비로 정규화

모든 클립을 60프레임 × 63차원 시퀀스로 변환

음성 (audio)

mp4에서 오디오 추출 → 무음 제거 및 정규화

16kHz, 64 Mel-bin 기준 60프레임 × 64차원 Mel-spectrogram으로 변환

3️⃣ GRU 기반 조기/후기 융합 학습

Early Fusion GRU

손(60×63) + 음성(60×64)을 각각 GRU로 인코딩 후,
특징을 합쳐 하나의 네트워크에서 이동·행동을 동시에 예측

Late Fusion GRU

손 / 음성을 각각 GRU + 분류기로 처리하고,
인코더 출력을 합쳐 후기 융합 head로 다시 분류

공통 지표:

Move Accuracy (이동 정확도)

Act Accuracy (행동 정확도)

C_fusion에서 이동·행동을 둘 다 맞춘 Pair-Accuracy(CSR)

4️⃣ 실시간 데모 & Unity 연동

웹캠 → MediaPipe → 최근 60프레임을 GRU(Early/Late)에 입력

마이크 음성은 별도 ASR 스크립트에서 키워드(“앞으로”, “공격”, “방어” 등)를 인식

Python에서 UDP 텍스트 명령 전송:

이동: G:FORWARD / G:BACKWARD / G:LEFT / G:RIGHT / G:STOP

액션: G:ATTACK / G:DEFEND (제스처), V:ATTACK / V:DEFEND (음성)

Unity CommandController가 이 명령을 받아

캐릭터 이동

공격(빨간색), 방어(파란색) 등 상태 시각화 및 애니메이션 트리거

⚙️ 시스템 아키텍처

입력

웹캠 영상 (손 제스처)

마이크 / 영상 내 오디오 (음성)

모델 파이프라인 (Python / PyTorch)

MediaPipe 기반 손 랜드마크 추출 및 정규화

오디오 → Mel-spectrogram 변환

GRU 기반 Early / Late Fusion 모델 학습 및 추론

실시간 연동

Python → Unity 간 UDP 통신 (텍스트 프로토콜)

Unity 내 캐릭터 컨트롤, 색상/상태 표현, 카메라 추적

📊 실험 및 주요 결과

동일한 train/val/test split에서 Early / Late Fusion을 비교하였다.

조기 융합 (Early Fusion) GRU

이동 정확도: ~99%

행동 정확도: ~83%

C_fusion CSR: 약 0.81

후기 융합 (Late Fusion) GRU

이동 정확도: ~97%

행동 정확도: ~80%

C_fusion CSR: 약 0.80

요약하면,

단일 지표(이동/행동 정확도) 기준으로는
Early Fusion이 약간 더 높은 경향이 있고,

쌍으로 완전히 맞춘 C_fusion CSR 기준에서는
두 방식 모두 비슷한 수준의 성능을 보인다.

실제 환경에서는 한 모달(손 또는 음성)이 약해지는 경우가 있어,
구조적으로 더 유연한 Late Fusion + 규칙 기반 결합을 함께 사용하는 전략이 유효하다.


✔️ 결론

손 제스처와 음성을 결합한 다중 모달 융합이
단일 모달보다 더 직관적이고 풍부한 인터랙션을 제공할 수 있음을 확인했다.

동일한 데이터셋에서 Early / Late Fusion을 비교함으로써,

Early Fusion: 단일 모델 구조, 약간 더 높은 평균 정확도

Late Fusion: 모달리티 손실/노이즈에 더 유연한 구조
라는 특성을 정리할 수 있었다.
