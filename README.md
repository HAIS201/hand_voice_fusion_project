손 제스처와 음성 인식을 활용한 다중 모달 융합 연구

## 📌 프로젝트 소개

게임, VR, 교육 등의 분야에서 인간–컴퓨터 상호작용(HCI)은 점점 더 복잡해지고 있으며,  
사용자는 단순한 키보드·마우스 입력이 아니라 신체 동작과 음성을 동시에 활용한 자연스러운 인터페이스를 요구하고 있다.

손 제스처는 사용자의 공간적 위치·방향·형태를 직관적으로 표현하는 데 강점이 있고,  
음성은 “공격해”, “방어해”와 같은 고수준 추상 명령을 빠르게 전달하는 데 적합하다.  
따라서 두 가지 모달리티(손 + 음성)를 함께 사용하는 **다중 모달 융합** 방식은  
단일 모달 인터페이스보다 더 직관적이고 몰입감 있는 사용자 경험을 제공할 수 있다.

본 프로젝트의 목표는 다음과 같다.

- 웹캠 + 마이크를 이용해  
  - 오른손 제스처로 이동(앞/뒤/좌/우/정지)  
  - 왼손 제스처와 음성으로 행동(공격/방어) 제어
- 동일한 데이터셋에서 **조기 융합(Early Fusion)** 과 **후기 융합(Late Fusion)** GRU 모델을 학습
- 두 융합 방식의 특성과 성능 차이를 비교·분석하고, Unity 데모로 실제 상호작용까지 구현

---

## 🔎 전체 동작 흐름

1. **데이터 수집 & 라벨링**
   - 직접 촬영한 mp4 영상을 세 가지 subset으로 구성
     - **A_hand** : 손 제스처만 존재 (이동/행동 단일 모달)
     - **B_voice**: 음성만 존재 (ATTACK / DEFEND)
     - **C_fusion**: 손 + 음성이 동시에 존재 (이동 + 행동 페어)
   - 밝기/소음 조합에 따라 `E1~E4` 환경 라벨 부여  
     (밝은/어두운 + 조용함/선풍기 소음)
   - `labels.csv`, `splits/train|val|test.csv` 로 전체 메타데이터 관리

2. **특징 추출 (전처리)**  
   - **손 제스처 (gesture)**
     - MediaPipe Hands → 21개 랜드마크 (x, y, z) 추출  
     - 손목(landmark 0)을 원점으로 평행이동  
     - 손바닥 너비(landmark 5–17 거리)로 정규화  
     - 각 프레임을 63차원 벡터로 만들고, 클립을 **60프레임 × 63차원** 시퀀스로 변환
   - **음성 (audio)**
     - mp4에서 오디오 추출, 정규화 및 무음 구간 처리  
     - 16kHz 샘플링, 64 Mel-bin 기준 **60프레임 × 64차원 Mel-spectrogram** 시퀀스로 변환
   - 최종적으로 각 샘플은  
     - 제스처: (60, 63)  
     - 음성: (60, 64)  
     형태의 시퀀스 입력으로 사용

3. **GRU 기반 조기 / 후기 융합 학습**
   - 공통적으로 PyTorch 기반 **양방향 GRU 인코더** 사용
   - **Early Fusion GRU**
     - 제스처(60×63)와 음성(60×64)을 각각 GRU로 인코딩  
     - 두 인코더 출력을 이어 붙여 하나의 fusion feature 생성  
     - 이 feature로부터 **이동(Move)** 과 **행동(Act)** 을 동시에 예측
   - **Late Fusion GRU**
     - 제스처 전용 GRU + 이동 head  
     - 음성 전용 GRU + 행동 head  
     - 인코더 출력을 합친 **fusion head** 를 추가로 두고,  
       C_fusion에서는 손+음성을 함께 보는 후기 융합 분류 수행
   - 공통 평가 지표
     - **Move Accuracy** : 이동 클래스(FORWARD/BACKWARD/LEFT/RIGHT/STOP) 정확도
     - **Act Accuracy**   : 행동 클래스(ATTACK/DEFEND) 정확도
     - **C_fusion CSR**   : C_fusion 샘플에서 이동·행동 페어를 둘 다 맞춘 비율  
       (Command Success Rate, Pair-Accuracy)

4. **실시간 데모 & Unity 연동**
   - **실시간 제스처 인식**
     - 웹캠 → MediaPipe → 최근 60프레임 버퍼를 실시간으로 갱신  
     - 버퍼가 채워지면 Early/Late Fusion GRU에 입력 → 이동/행동 추론
   - **실시간 음성 인식**
     - 별도 Python 스크립트에서 마이크 입력을 받아  
       “앞으로 / 뒤로 / 왼쪽 / 오른쪽 / 공격 / 방어” 등의 키워드 인식
   - **Python → Unity UDP 통신**
     - 이동: `G:FORWARD / G:BACKWARD / G:LEFT / G:RIGHT / G:STOP`  
     - 행동(제스처): `G:ATTACK / G:DEFEND`  
     - 행동(음성):   `V:ATTACK / V:DEFEND`
   - **Unity 데모**
     - `CommandController` 스크립트가 명령을 받아
       - 캐릭터 이동 (지속적인 방향 이동)
       - 공격 시 빨간색, 방어 시 파란색으로 캐릭터 색상 변경
       - 점프 등 추가 액션 처리
     - 카메라는 플레이어를 따라가도록 설정하여,  
       손/음성으로 직접 캐릭터를 움직이는 경험 제공

---

## ⚙️ 시스템 아키텍처 요약

- **입력**
  - 웹캠 영상 → 손 제스처
  - 마이크 / 영상 내 오디오 → 음성 명령
- **모델 파이프라인 (Python / PyTorch)**
  - MediaPipe 기반 손 랜드마크 추출 및 정규화
  - 오디오 → Mel-spectrogram 변환
  - GRU 기반 Early / Late Fusion 모델 학습 및 추론
- **실시간 연동**
  - Python ↔ Unity 간 UDP 텍스트 프로토콜 사용
  - Unity에서 캐릭터 이동, 색상·상태 시각화, 카메라 추적

---

## 📊 실험 설정 & 주요 결과

동일한 train/val/test split과 동일한 특징(손 + 음성)을 사용하여  
조기 융합 GRU와 후기 융합 GRU를 비교하였다.

### ✅ 조기 융합 (Early Fusion GRU)

- Move Accuracy : **0.9909**
- Act Accuracy  : **0.8295**
- C_fusion CSR  : **0.8125**
- 4가지 환경(E1~E4) 모두에서 비교적 안정적인 성능 유지

### ✅ 후기 융합 (Late Fusion GRU)

- Move Accuracy : **0.9727**
- Act Accuracy  : **0.7955**
- C_fusion CSR  : **0.8000**
- 손/음성 인코더를 분리해 둔 덕분에  
  한 모달에 노이즈가 큰 환경에서도 구조적으로 대응이 쉬움

### 📌 Early vs Late Fusion 비교 요약

- 단일 지표(이동/행동 정확도) 기준으로는  
  **Early Fusion이 약간 더 높은 평균 성능**을 보였다.
- C_fusion에서 이동·행동을 페어로 모두 맞춰야 하는 **CSR 기준**에서는  
  두 방식이 **거의 비슷한 수준**의 성능을 보였다.
- 실제 사용 환경에서는
  - 손 카메라가 가려지거나,
  - 소음으로 인해 음성 인식이 불안정해지는 상황 등이 자주 발생하므로,
  - **Late Fusion 구조 + 규칙 기반 결합**을 함께 사용하는 전략이  
    모듈 교체/추가 측면에서 더 유연하다는 점을 확인했다.

---

## ✔️ 결론 및 정리

- 손 제스처와 음성을 결합한 **다중 모달 융합 인터페이스**가  
  단일 모달(손만, 음성만)보다 더 직관적이고 풍부한 상호작용을 제공할 수 있음을 확인했다.
- 동일한 데이터셋에서 **조기(Early) / 후기(Late) 융합 GRU**를 비교한 결과,
  - Early Fusion : 단일 모델 구조, 조금 더 높은 평균 정확도
  - Late Fusion  : 모달리티 손실·노이즈 상황에 더 유연한 구조
  라는 특성을 정리할 수 있었다.
- 최종적으로,  
  **“모델 성능만 보면 Early Fusion, 실제 시스템 확장성과 안정성을 고려하면 Late Fusion + 규칙 기반 결합도 충분히 경쟁력 있음”**  
  이라는 결론에 도달하였다.

이 저장소는 위 과정을 모두 재현할 수 있는  
데이터 전처리, GRU 학습 스크립트, 그리고 Unity 연동 데모 코드를 포함한다.
```
